{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Repo Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/Drwaish/TalkingAvatar.git\n",
        "%cd TalkingAvatar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDA9KU5ZaIlv",
        "outputId": "3c67d1af-8407-40b4-e204-af891575c3b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nY-1T4LELlGY"
      },
      "outputs": [],
      "source": [
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS3yLsoGoo0t",
        "outputId": "1e5a020e-e6d7-4ff3-c0aa-8c15407f8a1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1tvI43ZIrnx9Ti2TpFiEO4dK5DOwcECD7\n",
            "To: /content/Audio2Head/checkpoints/audio2head.pth.tar\n",
            "100% 756M/756M [00:12<00:00, 60.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/u/0/uc?id=1tvI43ZIrnx9Ti2TpFiEO4dK5DOwcECD7&export=download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR8r_VN8LEST",
        "outputId": "cf374fdb-27bc-429e-fb17-d69d233bce87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Audio2Head\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Audio to Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nhVm7F4DLtvQ"
      },
      "outputs": [],
      "source": [
        "from gtts import gTTS\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZAksTjLOyqdo"
      },
      "outputs": [],
      "source": [
        "def text_to_audio(mytext : str):\n",
        "    try:\n",
        "        # Language in which you want to convert\n",
        "        language = 'en'\n",
        "\n",
        "        # Passing the text and language to the engine,\n",
        "        # here we have marked slow=False. Which tells\n",
        "        # the module that the converted audio should\n",
        "        # have a high speed\n",
        "        myobj = gTTS(text=mytext, lang=language, slow=False)\n",
        "\n",
        "        # Saving the converted audio in a mp3 file named\n",
        "        # welcome\n",
        "        myobj.save(\"audio/input.wav\")\n",
        "        return True\n",
        "    except FileNotFoundError as error:\n",
        "        print(\"Error text to audio\", error)\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtDnabEnMrCD",
        "outputId": "5a919feb-f4fc-45ae-baed-1b8af8d8b07b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://www.google.com/imgres?imgurl=https%3A%2F%2Fpbs.twimg.com%2Fmedia%2FFjLZiZ_acAIq-lf.jpg\n",
            "To: /content/Audio2Head/imgres?imgurl=https%3A%2F%2Fpbs.twimg.com%2Fmedia%2FFjLZiZ_acAIq-lf.jpg\n",
            "\r0.00B [00:00, ?B/s]\r63.0kB [00:00, 59.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# !gdown https://www.google.com/imgres?imgurl=https%3A%2F%2Fpbs.twimg.com%2Fmedia%2FFjLZiZ_acAIq-lf.jpg&tbnid=EfmxwIQAF_UyzM&vet=12ahUKEwjaj8jdhKCCAxXmXqQEHRFgBRUQMyhCegUIARDvAQ..i&imgrefurl=https%3A%2F%2Ftwitter.com%2Faslanpahari%2Fstatus%2F1599580007067947008&docid=Ttkoa_I_Pv-rcM&w=1024&h=1024&q=ai%20generated%20images&ved=2ahUKEwjaj8jdhKCCAxXmXqQEHRFgBRUQMyhCegUIARDvAQ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OpenAI Chat Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "JSOFlSA7z5cU"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "## if user using goole colab\n",
        " \n",
        "# from google.colab import userdata\n",
        "# userdata.get('OPENAI_API_KEY')\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QR1UcODHIo7l"
      },
      "outputs": [],
      "source": [
        "def question_answer(messages):\n",
        "    response = openai.ChatCompletion.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=messages,\n",
        "      temperature=1,\n",
        "      max_tokens=256,\n",
        "      top_p=1,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0\n",
        "    )\n",
        "    return response['choices'][0]['message']['content']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VGgLaa_UI_wZ"
      },
      "outputs": [],
      "source": [
        "messages=[\n",
        "        {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"Answer the message enter by user.\"\n",
        "        },\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "        },\n",
        "      ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hdca7YLmJXza"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GSk25iT2O_NL"
      },
      "outputs": [],
      "source": [
        "import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Rr3d1Co2M99m"
      },
      "outputs": [],
      "source": [
        "def complete_data(text):\n",
        "    messages[1]['content'] = text\n",
        "    my_text = question_answer(messages)\n",
        "    print(my_text)\n",
        "    if my_text is not None:\n",
        "        check = text_to_audio(my_text)\n",
        "        if check:\n",
        "          os.system(\"rm -rf results/image_input.mp4\")  # if file already exist , delete this file\n",
        "          var = subprocess.call(\"python inference.py --audio_path audio/input.wav --img_path avatar/image.jpg\", shell = True)\n",
        "          print(var)\n",
        "    if var==0:\n",
        "      return \"results/image_input.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Front End using Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "YN4fIOLszdrd",
        "outputId": "f50f322d-03ac-473c-8143-fdb569607171"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://1a12783983907fbb15.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://1a12783983907fbb15.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! I'm an AI, so I don't have feelings, but thank you for asking. How can I assist you today?\n",
            "0\n",
            "Pakistan is a country located in South Asia with a population of over 220 million people. It is known for its diverse culture, rich history, beautiful landscapes, and is home to some of the world's highest mountains, including K2.\n",
            "0\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://1a12783983907fbb15.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with gr.Blocks() as demo:\n",
        "  text_box = gr.Textbox(label = \"Enter your message here\")\n",
        "  avatar_response = gr.Video(label = \"Avatar\")\n",
        "  text_box.submit(fn = complete_data, inputs = text_box, outputs = avatar_response)\n",
        "\n",
        "demo.launch(debug = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# END Have a Nice Day"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
